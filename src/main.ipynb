{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f8660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ Nepali Grammar Error Correction Training\n",
      "============================================================\n",
      "Model: google/mt5-small\n",
      "LoRA: True\n",
      "Samples: 15\n",
      "============================================================\n",
      "‚úÖ Seeds set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared\n",
      "Directories created in ../outputs\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bertscore_f1</td><td>‚ñà‚ñÅ‚ñà</td></tr><tr><td>eval/bleu</td><td>‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/chrf</td><td>‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/correction_accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/gleu</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÅ‚ñÇ</td></tr><tr><td>eval/model_preparation_time</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñà‚ñà</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bertscore_f1</td><td>0.44887</td></tr><tr><td>eval/bleu</td><td>0.85086</td></tr><tr><td>eval/chrf</td><td>0.08999</td></tr><tr><td>eval/correction_accuracy</td><td>0</td></tr><tr><td>eval/gleu</td><td>0.0098</td></tr><tr><td>eval/loss</td><td>15.7358</td></tr><tr><td>eval/model_preparation_time</td><td>0.002</td></tr><tr><td>eval/runtime</td><td>15.6512</td></tr><tr><td>eval/samples_per_second</td><td>0.192</td></tr><tr><td>eval/steps_per_second</td><td>0.064</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-mountain-66</strong> at: <a href='https://wandb.ai/lsumit008-khwopa-college-of-engineering/nepali-grammar-correction/runs/4lb4qbnm' target=\"_blank\">https://wandb.ai/lsumit008-khwopa-college-of-engineering/nepali-grammar-correction/runs/4lb4qbnm</a><br> View project at: <a href='https://wandb.ai/lsumit008-khwopa-college-of-engineering/nepali-grammar-correction' target=\"_blank\">https://wandb.ai/lsumit008-khwopa-college-of-engineering/nepali-grammar-correction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251117_234758-4lb4qbnm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\logging\\__init__.py\", line 1153, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode characters in position 187-191: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\base_events.py\", line 678, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\base_events.py\", line 2033, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\events.py\", line 89, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_23156\\3701139412.py\", line 113, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_23156\\3701139412.py\", line 46, in main\n",
      "    wandb.init(\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1582, in init\n",
      "    run = wi.init(run_settings, run_config, run_printer)\n",
      "  File \"c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 837, in init\n",
      "    self._logger.info(\n",
      "Message: \"wandb.init called with sweep_config: {}\\nconfig: {'model_id': 'google/mt5-small', 'max_length': 64, 'prefix': '‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§∏‡§ö‡•ç‡§Ø‡§æ‡§â‡§®‡•Å‡§π‡•ã‡§∏‡•ç: ', 'dataset_name': 'sumitaryal/nepali_grammatical_error_correction', 'num_samples': 15, 'test_size': 0.2, 'use_lora': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'lora_target_modules': ['q', 'k', 'v', 'o', 'wi', 'wo'], 'batch_size': 16, 'num_epochs': 5, 'gradient_accumulation_steps': 2, 'learning_rate': 0.0003, 'weight_decay': 0.01, 'warmup_ratio': 0.05, 'use_8bit': True, 'use_fp16': False, 'gradient_checkpointing': True, 'output_dir': '../outputs', 'logging_steps': 10, 'save_total_limit': 2, 'early_stopping_patience': 3, 'wandb_project': 'nepali-grammar-correction', 'seed': 42, '_wandb': {}}\"\n",
      "Arguments: ()\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\src\\wandb\\run-20251117_235107-gs0yfqs5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lsumit008-khwopa-college-of-engineering/nepali-grammar-correction/runs/gs0yfqs5' target=\"_blank\">devout-moon-67</a></strong> to <a href='https://wandb.ai/lsumit008-khwopa-college-of-engineering/nepali-grammar-correction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lsumit008-khwopa-college-of-engineering/nepali-grammar-correction' target=\"_blank\">https://wandb.ai/lsumit008-khwopa-college-of-engineering/nepali-grammar-correction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lsumit008-khwopa-college-of-engineering/nepali-grammar-correction/runs/gs0yfqs5' target=\"_blank\">https://wandb.ai/lsumit008-khwopa-college-of-engineering/nepali-grammar-correction/runs/gs0yfqs5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö Loading dataset: sumitaryal/nepali_grammatical_error_correction\n",
      "  Using 15 samples\n",
      "  Train: 12 samples\n",
      "  Valid: 3 samples\n",
      "\n",
      " Lodaing model: google/mt5-small\n",
      " Using LoRA + 8-bit quantization\n",
      "trainable params: 1,769,472 || all params: 301,946,240 || trainable%: 0.5860\n",
      "\n",
      "‚öôÔ∏è  Preprocessing dataset...\n",
      "  ‚úÖ Preprocessing complete\n",
      "\n",
      "üìä Training plan:\n",
      "  Steps per epoch: 1\n",
      "  Total steps: 5\n",
      "  Warmup steps: 0\n",
      "\n",
      " Running pre-training safety checks...\n",
      "Model device: cuda:0\n",
      "Train dataset size: 12\n",
      "Eval dataset size: 3\n",
      " Data loading works\n",
      " Performing evaluation check...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Sample - Pred: '<extra_id_0>...' | Ref: '‡§´‡§∞‡•ç‡§ï‡§®‡•á ‡§π‡•ã ‡§â‡§§‡•à?...' | Match: False\n",
      " Evaluation successful\n",
      "Initial metrics: {'eval_loss': 15.911977767944336, 'eval_model_preparation_time': 0.0023, 'eval_bleu': 0.8508564639341064, 'eval_chrf': 0.08999280057595392, 'eval_correction_accuracy': 0.0, 'eval_bertscore_f1': 0.4488728940486908, 'eval_gleu': 0.00980392156862745, 'eval_runtime': 17.2188, 'eval_samples_per_second': 0.174, 'eval_steps_per_second': 0.058}\n",
      "\n",
      "============================================================\n",
      "üèãÔ∏è  Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/5 00:30 < 01:01, 0.03 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>Correction Accuracy</th>\n",
       "      <th>Bertscore F1</th>\n",
       "      <th>Gleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>15.697292</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.011845</td>\n",
       "      <td>0.179986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442217</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>15.735796</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.850856</td>\n",
       "      <td>0.089993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448873</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Sample - Pred: '<extra_id_0>...' | Ref: '‡§´‡§∞‡•ç‡§ï‡§®‡•á ‡§π‡•ã ‡§â‡§§‡•à?...' | Match: False\n",
      "üîç Sample - Pred: '<extra_id_0>...' | Ref: '‡§´‡§∞‡•ç‡§ï‡§®‡•á ‡§π‡•ã ‡§â‡§§‡•à?...' | Match: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m     wandb.finish()\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\transformers\\trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\transformers\\trainer.py:2623\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2621\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2622\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2623\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2625\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2626\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\transformers\\trainer.py:5581\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5579\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5580\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5582\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5583\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\accelerate\\data_loader.py:564\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28mself\u001b[39m.begin()\n\u001b[32m    563\u001b[39m \u001b[38;5;28mself\u001b[39m.set_epoch(\u001b[38;5;28mself\u001b[39m.iteration)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m dataloader_iter = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_dataloader\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:494\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:427\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\Nepali_GEC\\nepali_gec\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1170\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1163\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1164\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\context.py:337\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\popen_spawn_win32.py:97\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     96\u001b[39m     reduction.dump(prep_data, to_child)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     99\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from math import ceil\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Import our custom modules\n",
    "from config import Config\n",
    "from metrics import create_compute_metrics\n",
    "from data_utils import set_seeds, load_and_prepare_dataset, preprocess_dataset\n",
    "from utils import clear_memory, create_directories, safe_training_check, save_model_safe\n",
    "from train import setup_model, create_training_args\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    \n",
    "    # Load config\n",
    "    config = Config()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üöÄ Nepali Grammar Error Correction Training\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model: {config.model_id}\")\n",
    "    print(f\"LoRA: {config.use_lora}\")\n",
    "    print(f\"Samples: {config.num_samples or 'Full dataset'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Setup\n",
    "    set_seeds(config.seed)\n",
    "    clear_memory()\n",
    "    create_directories(config.output_dir)\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.finish()\n",
    "    wandb.init(\n",
    "        project=config.wandb_project,\n",
    "        config=vars(config)\n",
    "    )\n",
    "    run_id = wandb.run.id\n",
    "    \n",
    "    # Load data\n",
    "    dataset = load_and_prepare_dataset(config)\n",
    "    \n",
    "    # Setup model\n",
    "    model, tokenizer = setup_model(config)\n",
    "    \n",
    "    # Preprocess\n",
    "    dataset_encoded = preprocess_dataset(dataset, tokenizer, config)\n",
    "    \n",
    "    # Create training args\n",
    "    training_args = create_training_args(config, dataset_encoded, run_id)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Create metrics\n",
    "    compute_metrics = create_compute_metrics(tokenizer)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_encoded[\"train\"],\n",
    "        eval_dataset=dataset_encoded[\"valid\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "        # Safety check\n",
    "    if not safe_training_check(trainer):\n",
    "        print(\"\\n‚ùå Safety checks failed! Fix issues before training.\")\n",
    "        return\n",
    "    \n",
    "    # Train!\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üèãÔ∏è  Starting training...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"\\n‚úÖ Training complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Training failed: {e}\")\n",
    "        wandb.finish()\n",
    "        return\n",
    "    \n",
    "    # Save model\n",
    "    best_model_path = f\"{config.output_dir}/best_model\"\n",
    "    save_model_safe(model, tokenizer, best_model_path, use_lora=config.use_lora)\n",
    "    \n",
    "    print(f\"\\nüéâ All done! Model saved to {best_model_path}\")\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b226c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
