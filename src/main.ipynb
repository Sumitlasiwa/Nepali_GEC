{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8660f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OutStream' object has no attribute 'reconfigure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Force UTF-8 encoding\u001b[39;00m\n\u001b[32m      6\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mPYTHONUTF8\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreconfigure\u001b[49m(encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m sys.stderr.reconfigure(encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# For Windows specifically\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'OutStream' object has no attribute 'reconfigure'"
     ]
    }
   ],
   "source": [
    "                \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import wandb\n",
    "from math import ceil\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "# Import our custom modules\n",
    "from config import Config\n",
    "from metrics import create_compute_metrics\n",
    "from data_utils import set_seeds, load_and_prepare_dataset, preprocess_dataset\n",
    "from utils import clear_memory, create_directories, safe_training_check, save_model_safe\n",
    "from train import setup_model, create_training_args\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    \n",
    "    # Load config\n",
    "    config = Config()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üöÄ Nepali Grammar Error Correction Training\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model: {config.model_id}\")\n",
    "    print(f\"LoRA: {config.use_lora}\")\n",
    "    print(f\"Samples: {config.num_samples or 'Full dataset'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Setup\n",
    "    set_seeds(config.seed)\n",
    "    clear_memory()\n",
    "    create_directories(config.output_dir)\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.finish()\n",
    "    wandb.init(\n",
    "        project=config.wandb_project,\n",
    "        config=vars(config)\n",
    "    )\n",
    "    run_id = wandb.run.id\n",
    "    \n",
    "    # Load data\n",
    "    dataset = load_and_prepare_dataset(config)\n",
    "    \n",
    "    # Setup model\n",
    "    model, tokenizer = setup_model(config)\n",
    "    \n",
    "    # Preprocess\n",
    "    dataset_encoded = preprocess_dataset(dataset, tokenizer, config)\n",
    "    \n",
    "    # Create training args\n",
    "    training_args = create_training_args(config, dataset_encoded, run_id)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Create metrics\n",
    "    compute_metrics = create_compute_metrics(tokenizer, config)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_encoded[\"train\"],\n",
    "        eval_dataset=dataset_encoded[\"valid\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "        # Safety check\n",
    "    if not safe_training_check(trainer):\n",
    "        print(\"\\n‚ùå Safety checks failed! Fix issues before training.\")\n",
    "        return\n",
    "    \n",
    "    # Train!\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üèãÔ∏è  Starting training...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        if config.resume_from_checkpoint:\n",
    "            print(\"continuing training from latest checkpoint.....\")\n",
    "            trainer.train(resume_from_checkpoint=True)\n",
    "        else:\n",
    "            trainer.train()\n",
    "        print(\"\\n‚úÖ Training complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Training failed: {e}\")\n",
    "        wandb.finish()\n",
    "        return\n",
    "    \n",
    "    # Save model\n",
    "    best_model_path = f\"{config.output_dir}/best_model\"\n",
    "    save_model_safe(model, tokenizer, best_model_path, use_lora=config.use_lora)\n",
    "    \n",
    "    print(f\"\\nüéâ All done! Model saved to {best_model_path}\")\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b226c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
